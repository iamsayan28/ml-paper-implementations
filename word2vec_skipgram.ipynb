{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efe74ca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:30.994471Z",
     "iopub.status.busy": "2025-05-21T10:09:30.994274Z",
     "iopub.status.idle": "2025-05-21T10:09:36.236207Z",
     "shell.execute_reply": "2025-05-21T10:09:36.235595Z"
    },
    "papermill": {
     "duration": 5.247615,
     "end_time": "2025-05-21T10:09:36.237836",
     "exception": false,
     "start_time": "2025-05-21T10:09:30.990221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf10933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.244578Z",
     "iopub.status.busy": "2025-05-21T10:09:36.244158Z",
     "iopub.status.idle": "2025-05-21T10:09:36.249137Z",
     "shell.execute_reply": "2025-05-21T10:09:36.248387Z"
    },
    "papermill": {
     "duration": 0.009058,
     "end_time": "2025-05-21T10:09:36.250186",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.241128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"models\"\n",
    "RAW_TEXT_PATH = os.path.join(DATA_DIR, \"processed_text.txt\")\n",
    "MAX_VOCAB_SIZE = 30000 \n",
    "MIN_COUNT = 5  \n",
    "MAX_TOKENS = 200000  \n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "ensure_dir(DATA_DIR)\n",
    "ensure_dir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99be8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.256122Z",
     "iopub.status.busy": "2025-05-21T10:09:36.255571Z",
     "iopub.status.idle": "2025-05-21T10:09:36.259686Z",
     "shell.execute_reply": "2025-05-21T10:09:36.258961Z"
    },
    "papermill": {
     "duration": 0.008053,
     "end_time": "2025-05-21T10:09:36.260723",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.252670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[,.;:\\(\\)\\[\\]{}\"\\'\\-_!?@#$%^&*+=<>~/\\\\|]', ' ', text)\n",
    "    \n",
    "    tokens = [token for token in text.split() if len(token) >= 2 and not token.isdigit()]\n",
    "    \n",
    "    return tokens[:MAX_TOKENS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593050d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.266523Z",
     "iopub.status.busy": "2025-05-21T10:09:36.266322Z",
     "iopub.status.idle": "2025-05-21T10:09:36.271355Z",
     "shell.execute_reply": "2025-05-21T10:09:36.270696Z"
    },
    "papermill": {
     "duration": 0.009153,
     "end_time": "2025-05-21T10:09:36.272404",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.263251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_small_dataset():\n",
    "    if os.path.exists(RAW_TEXT_PATH):\n",
    "        print(\"Loading cached text data...\")\n",
    "        with open(RAW_TEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        print(\"Creating sample text data...\")\n",
    "            \n",
    "        sample_text = \"\"\"\n",
    "        Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "        concerned with the interactions between computers and human language, in particular how to program computers \n",
    "        to process and analyze large amounts of natural language data. The goal is a computer capable of understanding \n",
    "        the contents of documents, including the contextual nuances of the language within them. The technology can then \n",
    "        accurately extract information and insights contained in the documents as well as categorize and organize the \n",
    "        documents themselves.\n",
    "        \n",
    "        Challenges in natural language processing frequently involve speech recognition, natural language understanding, \n",
    "        and natural language generation. Modern NLP algorithms are based on machine learning, especially statistical \n",
    "        machine learning and deep learning methods. Many different classes of machine-learning algorithms have been \n",
    "        applied to natural-language-processing tasks. These algorithms take as input a large set of features that are \n",
    "        generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems \n",
    "        of hard if-then rules similar to existing hand-written rules. Increasingly, however, research has focused on \n",
    "        statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each \n",
    "        input feature.\n",
    "        \n",
    "        Word embeddings are one of the most popular representations of document vocabulary. They are capable of capturing \n",
    "        context of a word in a document, semantic and syntactic similarity, relation with other words, etc. There are \n",
    "        several methods to train and extract word embeddings from text, including Word2Vec, GloVe, and FastText. These \n",
    "        methods are based on the distributional hypothesis, which states that words that occur in the same contexts tend \n",
    "        to have similar meanings. This allows these models to learn word representations that capture meaningful semantic \n",
    "        relationships between words, which can be used for various NLP tasks such as sentiment analysis, named entity \n",
    "        recognition, and machine translation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add it multiple times to get more data\n",
    "        text = sample_text * 20  \n",
    "        \n",
    "        with open(RAW_TEXT_PATH, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327258b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.278086Z",
     "iopub.status.busy": "2025-05-21T10:09:36.277574Z",
     "iopub.status.idle": "2025-05-21T10:09:36.282121Z",
     "shell.execute_reply": "2025-05-21T10:09:36.281373Z"
    },
    "papermill": {
     "duration": 0.008414,
     "end_time": "2025-05-21T10:09:36.283172",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.274758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocab(tokens, min_count=MIN_COUNT, max_size=MAX_VOCAB_SIZE):\n",
    "\n",
    "    word_freq = Counter(tokens)\n",
    "    vocab_list = [word for word, freq in word_freq.most_common(max_size) \n",
    "                 if freq >= min_count]\n",
    "    \n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
    "    return word_to_idx, idx_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9bd9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.289089Z",
     "iopub.status.busy": "2025-05-21T10:09:36.288874Z",
     "iopub.status.idle": "2025-05-21T10:09:36.296002Z",
     "shell.execute_reply": "2025-05-21T10:09:36.295292Z"
    },
    "papermill": {
     "duration": 0.011413,
     "end_time": "2025-05-21T10:09:36.297120",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.285707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, tokens, word_to_idx, window_size=2, neg_samples=5):\n",
    "        self.tokens = tokens\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.neg_samples = neg_samples\n",
    "        \n",
    "        self.corpus = [word_to_idx[token] for token in tokens if token in word_to_idx]\n",
    "        \n",
    "        self._prepare_negative_sampling()\n",
    "        \n",
    "    def _prepare_negative_sampling(self):\n",
    "        count = Counter(self.corpus)\n",
    "        freq = np.zeros(len(self.word_to_idx))\n",
    "        \n",
    "        for idx, word_freq in count.items():\n",
    "            freq[idx] = word_freq\n",
    "        \n",
    "        freq = freq ** 0.75\n",
    "        self.neg_sampling_prob = freq / freq.sum()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - 2 * self.window_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.window_size\n",
    "        \n",
    "        center_word_idx = self.corpus[idx]\n",
    "        \n",
    "        context_indices = list(range(idx - self.window_size, idx)) + \\\n",
    "                          list(range(idx + 1, idx + self.window_size + 1))\n",
    "        \n",
    "        context_pos = np.random.choice(context_indices)\n",
    "        context_word_idx = self.corpus[context_pos]\n",
    "        \n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < self.neg_samples:\n",
    "            neg_idx = np.random.choice(len(self.neg_sampling_prob), p=self.neg_sampling_prob)\n",
    "            if neg_idx != center_word_idx and neg_idx != context_word_idx:\n",
    "                neg_samples.append(neg_idx)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(center_word_idx, dtype=torch.long),\n",
    "            torch.tensor(context_word_idx, dtype=torch.long),\n",
    "            torch.tensor(neg_samples, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe416f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.302771Z",
     "iopub.status.busy": "2025-05-21T10:09:36.302386Z",
     "iopub.status.idle": "2025-05-21T10:09:36.307901Z",
     "shell.execute_reply": "2025-05-21T10:09:36.307405Z"
    },
    "papermill": {
     "duration": 0.009497,
     "end_time": "2025-05-21T10:09:36.309036",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.299539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        self.center_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.context_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def forward(self, center_words, context_words, negative_words):\n",
    "        # Get embeddings for center and positive context words\n",
    "        center_emb = self.center_embeddings(center_words)  \n",
    "        context_emb = self.context_embeddings(context_words)  \n",
    "        \n",
    "        pos_score = torch.sum(center_emb * context_emb, dim=1)  \n",
    "        \n",
    "        # Get embeddings for negative samples\n",
    "        neg_emb = self.context_embeddings(negative_words) \n",
    "        \n",
    "        # Compute negative scores (batch matrix multiplication)\n",
    "        neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze()  \n",
    "        \n",
    "        # Apply loss function (negative sampling loss)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)\n",
    "        \n",
    "        return -(pos_loss + neg_loss).mean()\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"Return the trained word embeddings\"\"\"\n",
    "        return self.center_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa4859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.314778Z",
     "iopub.status.busy": "2025-05-21T10:09:36.314526Z",
     "iopub.status.idle": "2025-05-21T10:09:36.324929Z",
     "shell.execute_reply": "2025-05-21T10:09:36.324444Z"
    },
    "papermill": {
     "duration": 0.014609,
     "end_time": "2025-05-21T10:09:36.326010",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.311401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_word2vec(tokens, word_to_idx, embedding_dim=100, epochs=5, batch_size=512, \n",
    "                   window_size=2, neg_samples=5, learning_rate=0.002):\n",
    "    \"\"\"Train Word2Vec model with checkpointing\"\"\"\n",
    "    \n",
    "    checkpoint_path = os.path.join(MODEL_DIR, \"word2vec_checkpoint.pt\")\n",
    "    final_model_path = os.path.join(MODEL_DIR, \"word2vec_final.pt\")\n",
    "    \n",
    "    dataset = Word2VecDataset(\n",
    "        tokens=tokens,\n",
    "        word_to_idx=word_to_idx,\n",
    "        window_size=window_size,\n",
    "        neg_samples=neg_samples\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  \n",
    "    )\n",
    "    \n",
    "    vocab_size = len(word_to_idx)\n",
    "    model = SkipGram(vocab_size, embedding_dim)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    print(f\"Training on: {device}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, (centers, contexts, negatives) in enumerate(progress_bar):\n",
    "            centers = centers.to(device)\n",
    "            contexts = contexts.to(device)\n",
    "            negatives = negatives.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model(centers, contexts, negatives)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "            # Save checkpoint every 500 batches\n",
    "            if (batch_idx + 1) % 500 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Checkpoint saved at batch {batch_idx+1}\")\n",
    "        \n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(os.path.join(MODEL_DIR, 'training_loss.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc9dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.331653Z",
     "iopub.status.busy": "2025-05-21T10:09:36.331427Z",
     "iopub.status.idle": "2025-05-21T10:09:36.337650Z",
     "shell.execute_reply": "2025-05-21T10:09:36.336985Z"
    },
    "papermill": {
     "duration": 0.010326,
     "end_time": "2025-05-21T10:09:36.338821",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.328495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_word_analogies(model, word_to_idx, idx_to_word):\n",
    "    \"\"\"Evaluate word analogies like king - man + woman = queen\"\"\"\n",
    "    \n",
    "    embeddings = model.get_embeddings()\n",
    "    \n",
    "    test_cases = [\n",
    "        ('king', 'man', 'woman', 'queen'),\n",
    "        ('paris', 'france', 'italy', 'rome'),\n",
    "        ('good', 'better', 'bad', 'worse'),\n",
    "        ('big', 'bigger', 'small', 'smaller')\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for word_a, word_b, word_c, expected in test_cases:\n",
    "        if not all(word in word_to_idx for word in [word_a, word_b, word_c, expected]):\n",
    "            results.append(f\"Skipping {word_a} - {word_b} + {word_c} = ? (missing word(s) in vocabulary)\")\n",
    "            continue\n",
    "        \n",
    "        idx_a = word_to_idx[word_a]\n",
    "        idx_b = word_to_idx[word_b]\n",
    "        idx_c = word_to_idx[word_c]\n",
    "        \n",
    "        target_vec = embeddings[idx_b] - embeddings[idx_a] + embeddings[idx_c]\n",
    "        \n",
    "        sims = cosine_similarity([target_vec], embeddings)[0]\n",
    "        \n",
    "        closest_idxs = np.argsort(sims)[::-1][:5]\n",
    "        closest_words = [idx_to_word[idx] for idx in closest_idxs]\n",
    "        \n",
    "        closest_words = [w for w in closest_words if w not in [word_a, word_b, word_c]]\n",
    "        \n",
    "        expected_rank = closest_words.index(expected) + 1 if expected in closest_words else \"not found\"\n",
    "        \n",
    "        result = f\"{word_a} - {word_b} + {word_c} = {closest_words[0]} (expected: {expected}, rank: {expected_rank})\"\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c8b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.344629Z",
     "iopub.status.busy": "2025-05-21T10:09:36.344210Z",
     "iopub.status.idle": "2025-05-21T10:09:36.349334Z",
     "shell.execute_reply": "2025-05-21T10:09:36.348805Z"
    },
    "papermill": {
     "duration": 0.009204,
     "end_time": "2025-05-21T10:09:36.350469",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.341265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, word_to_idx, idx_to_word, num_words=50):\n",
    "    \"\"\"Visualize word embeddings using PCA\"\"\"\n",
    "    \n",
    "    embeddings = model.get_embeddings()\n",
    "    words = list(word_to_idx.keys())[:num_words]\n",
    "    word_vectors = np.array([embeddings[word_to_idx[word]] for word in words])\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(result[:, 0], result[:, 1], alpha=0.7)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=9)\n",
    "    \n",
    "    plt.title('Word Embeddings Visualization')\n",
    "    plt.savefig(os.path.join(MODEL_DIR, 'embeddings_visualization.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return os.path.join(MODEL_DIR, 'embeddings_visualization.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c808c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.356118Z",
     "iopub.status.busy": "2025-05-21T10:09:36.355928Z",
     "iopub.status.idle": "2025-05-21T10:09:36.360368Z",
     "shell.execute_reply": "2025-05-21T10:09:36.359847Z"
    },
    "papermill": {
     "duration": 0.008492,
     "end_time": "2025-05-21T10:09:36.361444",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.352952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar_words(model, word, word_to_idx, idx_to_word, top_n=10):\n",
    "    \"\"\"Find words similar to the input word\"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        return [f\"'{word}' not found in vocabulary\"]\n",
    "    \n",
    "    embeddings = model.get_embeddings()\n",
    "    \n",
    "    word_idx = word_to_idx[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    \n",
    "    sims = cosine_similarity([word_vec], embeddings)[0]\n",
    "    \n",
    "    # Get top N similar words\n",
    "    most_similar_idxs = np.argsort(sims)[::-1][:top_n+1]  # +1 because the word itself will be included\n",
    "    similar_words = [(idx_to_word[idx], sims[idx]) for idx in most_similar_idxs if idx != word_idx]\n",
    "    \n",
    "    return similar_words[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bd1f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T10:09:36.367243Z",
     "iopub.status.busy": "2025-05-21T10:09:36.366723Z",
     "iopub.status.idle": "2025-05-21T10:09:47.167990Z",
     "shell.execute_reply": "2025-05-21T10:09:47.167288Z"
    },
    "papermill": {
     "duration": 10.805261,
     "end_time": "2025-05-21T10:09:47.169120",
     "exception": false,
     "start_time": "2025-05-21T10:09:36.363859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing text data...\n",
      "Loading cached text data...\n",
      "Total tokens: 5860\n",
      "Vocabulary size: 164\n",
      "Training Word2Vec model...\n",
      "Training on: cpu\n",
      "Loading checkpoint...\n",
      "Resuming from epoch 5\n",
      "Final model saved to models\\word2vec_final.pt\n",
      "\n",
      "Evaluating word analogies:\n",
      "Skipping king - man + woman = ? (missing word(s) in vocabulary)\n",
      "Skipping paris - france + italy = ? (missing word(s) in vocabulary)\n",
      "Skipping good - better + bad = ? (missing word(s) in vocabulary)\n",
      "Skipping big - bigger + small = ? (missing word(s) in vocabulary)\n",
      "\n",
      "Finding similar words:\n",
      "Words similar to 'language':\n",
      "  processing: 0.8102\n",
      "  understanding: 0.8010\n",
      "  natural: 0.7473\n",
      "  generation: 0.7372\n",
      "  translation: 0.7341\n",
      "Words similar to 'learning':\n",
      "  especially: 0.7623\n",
      "  deep: 0.7515\n",
      "  train: 0.7392\n",
      "  machine: 0.7253\n",
      "  statistical: 0.7065\n",
      "Words similar to 'computer':\n",
      "  capturing: 0.8090\n",
      "  linguistics: 0.7688\n",
      "  capable: 0.7400\n",
      "  different: 0.7392\n",
      "  subfield: 0.7262\n",
      "Words similar to 'natural':\n",
      "  frequently: 0.7840\n",
      "  language: 0.7473\n",
      "  processing: 0.7262\n",
      "  understanding: 0.7224\n",
      "  translation: 0.7223\n",
      "\n",
      "Visualizing word embeddings...\n",
      "Visualization saved to models\\embeddings_visualization.png\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and processing text data...\")\n",
    "text = load_small_dataset()\n",
    "tokens = simple_tokenize(text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "word_to_idx, idx_to_word = build_vocab(tokens)\n",
    "\n",
    "print(\"Training Word2Vec model...\")\n",
    "embedding_dim = 100\n",
    "model = train_word2vec(\n",
    "    tokens=tokens,\n",
    "    word_to_idx=word_to_idx,\n",
    "    embedding_dim=embedding_dim,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    window_size=2,\n",
    "    neg_samples=5,\n",
    "    learning_rate=0.002\n",
    ")\n",
    "\n",
    "# Evaluate analogies\n",
    "print(\"\\nEvaluating word analogies:\")\n",
    "analogy_results = evaluate_word_analogies(model, word_to_idx, idx_to_word)\n",
    "for result in analogy_results:\n",
    "    print(result)\n",
    "\n",
    "print(\"\\nFinding similar words:\")\n",
    "for test_word in ['language', 'learning', 'computer', 'natural']:\n",
    "    if test_word in word_to_idx:\n",
    "        similar = find_similar_words(model, test_word, word_to_idx, idx_to_word, top_n=5)\n",
    "        print(f\"Words similar to '{test_word}':\")\n",
    "        for word, score in similar:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nVisualizing word embeddings...\")\n",
    "viz_path = visualize_embeddings(model, word_to_idx, idx_to_word)\n",
    "print(f\"Visualization saved to {viz_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb8d66",
   "metadata": {
    "papermill": {
     "duration": 0.014029,
     "end_time": "2025-05-21T10:09:47.198177",
     "exception": false,
     "start_time": "2025-05-21T10:09:47.184148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.130403,
   "end_time": "2025-05-21T10:09:49.133397",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T10:09:27.002994",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
